{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpNFmTkZY3l2ZQjpZgLWtJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koleshjr/ALL_MY_TEMPLATES/blob/main/Data_Driven_EDA_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA DRIVEN EDA"
      ],
      "metadata": {
        "id": "zWSlSRBrG3da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Z_bciofYK2J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Finding Missing values, Number of categorical and numerical columns"
      ],
      "metadata": {
        "id": "3MhVe5DfG5rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Common_data_analysis(data):\n",
        "    print(\"{:=^100}\".format(\" Common data analysis \"))\n",
        "    # default settings.\n",
        "    column = data.columns\n",
        "    total_samples = data.shape[0]\n",
        "    value_dict = {}\n",
        "    \n",
        "    # calculate values.\n",
        "    missing_values = data.isnull().sum().values\n",
        "    missing_value_percentage = [round((col_missing_count / total_samples) * 100, 2) for col_missing_count in missing_values ]\n",
        "    datatype = [data.iloc[:,i].dtype for i in range(data.shape[1])]\n",
        "    \n",
        "    categorical_data = list(data.loc[:,data.dtypes == 'object'].columns)\n",
        "    numerical_data = [d for d  in column if d not in categorical_data]\n",
        "    # print the diff datatype and count.\n",
        "    print()\n",
        "    print(\"Numerical data list {} ---> total {} numerical values\".format(numerical_data, len(numerical_data)))\n",
        "    print(\"Categorical data list {} ---> total {} categorical values\".format(categorical_data, len(categorical_data)))\n",
        "    print()\n",
        "    \n",
        "    # organise values.\n",
        "    value_dict[\"data type\"] = datatype\n",
        "    value_dict[\"Missing Value\"] = missing_values\n",
        "    value_dict[\"% of Missing value\"] = missing_value_percentage\n",
        "    df = pd.DataFrame(value_dict, columns = value_dict.keys(), index = column)\n",
        "    \n",
        "    # make a highlight for col has high missing value percentage. (>55% say)\n",
        "    # the particular row will be highlighted if it above missing value threshold.\n",
        "    def highlight_high_missing_value(sample):\n",
        "        threshold = 10.0\n",
        "        style = sample.copy()\n",
        "        highlight = 'background-color: red;'\n",
        "        if sample[2] > threshold:\n",
        "            style[:] = highlight\n",
        "        else:\n",
        "            style[:] = ''\n",
        "        return style\n",
        "    df = df.style.apply(highlight_high_missing_value, axis = 1)\n",
        "    display(df)\n",
        "    return (column, categorical_data, numerical_data)\n",
        "\n",
        "data_columns, categorical_data, numerical_data = Common_data_analysis(data)\n"
      ],
      "metadata": {
        "id": "2p7rojGUG5FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Column wise null values distribution"
      ],
      "metadata": {
        "id": "Q02D4zr9MB3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_null = pd.DataFrame(test.isna().sum())\n",
        "test_null = test_null.sort_values(by = 0 ,ascending = False)[:-5]\n",
        "train_null = pd.DataFrame(train.isna().sum())\n",
        "train_null = train_null.sort_values(by = 0 ,ascending = False)[:-6]\n",
        "fig, axes = plt.subplots(1,2, figsize=(18,10))\n",
        "sns.barplot( y =test_null.index ,  x  = test_null[0] ,ax = axes[1] ,palette = \"viridis\")\n",
        "sns.barplot( y =train_null.index ,  x  = train_null[0],ax = axes[0],palette = \"viridis\")\n",
        "axes[0].set_xlabel(\"TRAIN DATA COLUMNS\")\n",
        "axes[1].set_xlabel(\"TEST DATA COLUMNS\");"
      ],
      "metadata": {
        "id": "IDc42_OgMFDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Row wise null values distribution"
      ],
      "metadata": {
        "id": "DgoIXipMMJ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "missing_train_row = train.isna().sum(axis=1)\n",
        "missing_train_row = pd.DataFrame(missing_train_row.value_counts()/train.shape[0]).reset_index()\n",
        "missing_test_row = test.isna().sum(axis=1)\n",
        "missing_test_row = pd.DataFrame(missing_test_row.value_counts()/test.shape[0]).reset_index()\n",
        "missing_train_row.columns = ['no', 'count']\n",
        "missing_test_row.columns = ['no', 'count']\n",
        "missing_train_row[\"count\"] = missing_train_row[\"count\"]*100\n",
        "missing_test_row[\"count\"] = missing_test_row[\"count\"]*100\n",
        "fig, axes = plt.subplots(1,2, figsize=(18,6))\n",
        "sns.barplot( y =missing_train_row[\"count\"] ,  x  = missing_train_row[\"no\"],ax = axes[1] ,palette = \"viridis\")\n",
        "sns.barplot( y =missing_test_row[\"count\"] ,  x  = missing_test_row[\"no\"],ax = axes[0] ,palette = \"viridis\")\n",
        "axes[0].set_ylabel(\"Percentage of Null values\")\n",
        "axes[1].set_ylabel(\"Percentage of Null values\")\n",
        "axes[0].set_xlabel(\"TRAIN DATASET\")\n",
        "axes[1].set_xlabel(\"TEST DATASET\");\n",
        "\n"
      ],
      "metadata": {
        "id": "toefxQp9MNb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing values visualization"
      ],
      "metadata": {
        "id": "zF_fE9-SLU9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing value visualization.\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.heatmap(data.isnull().transpose(),\n",
        "            cbar_kws={'label': 'Missing Data'})\n",
        "plt.title('Heatmap showing Missing Values ', size = 20, color = 'red')\n",
        "plt.xticks(size = 12)\n",
        "plt.yticks(size = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3OGgV4Y6LQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical and Continuous diff distribution"
      ],
      "metadata": {
        "id": "hru17cgmMYtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)\n",
        "\n",
        "cat_features = [col for col in FEATURES if df[col].nunique() < 15]\n",
        "cont_features = [col for col in FEATURES if df[col].nunique() >= 15]\n",
        "\n",
        "del df\n",
        "print(f'Total number of features: {len(FEATURES)}')\n",
        "print(f'\\033[92mNumber of categorical features: {len(cat_features)}')\n",
        "print(f'\\033[96mNumber of continuos features: {len(cont_features)}')\n",
        "\n",
        "plt.pie([len(cat_features), len(cont_features)], \n",
        "        labels=['Categorical', 'Continuos'],\n",
        "        colors=['#DE3163', '#58D68D'],\n",
        "        textprops={'fontsize': 13},\n",
        "        autopct='%1.1f%%')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "knDc2O8pMgEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Numerical Data stats description"
      ],
      "metadata": {
        "id": "OViun_mWHmKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_data_analysis(num_data):\n",
        "  '''takes in a dataframe with every attribute of numerical data type'''\n",
        "    print(\"{:=^100}\".format(\" Numerical data analysis \"))\n",
        "\n",
        "    column = data.columns\n",
        "\n",
        "    min_value = [data[col].min() if col in numerical_data else \"NA\" for col in column]\n",
        "    max_value = [data[col].max() if col in numerical_data else \"NA\" for col in column]\n",
        "    #mode_value = [data[col].mode() if col in numerical_data else \"NA\" for col in column]\n",
        "    mean_value = [data[col].mean() if col in numerical_data else \"NA\" for col in column]\n",
        "    std_value = [data[col].std() if col in numerical_data else \"NA\" for col in column]\n",
        "    #print(mode_value)\n",
        "    skewness_value = [data[col].skew() if col in numerical_data else \"NA\" for col in column]\n",
        "    kurtosis_value = [data[col].kurtosis() if col in numerical_data else \"NA\" for col in column]\n",
        "\n",
        "    q1_value = [data[col].quantile(0.25) if col in numerical_data else \"NA\" for col in column]\n",
        "    q2_meadian_value = [data[col].quantile(0.50) if col in numerical_data else \"NA\" for col in column]\n",
        "    q3_value = [data[col].quantile(0.75) if col in numerical_data else \"NA\" for col in column]\n",
        "\n",
        "    # find the range value.\n",
        "    def find_range(min_value_list, max_value_list):\n",
        "        range_value = [(max_value - min_value)  if min_value != \"NA\" else \"NA\" for max_value, min_value in zip(max_value_list, min_value_list)]\n",
        "        return range_value\n",
        "\n",
        "    # find the inter quartile range. (q3-q1)\n",
        "    def iqr(q1_value_list, q3_value_list):\n",
        "        range_value = [(q3 - q1) if q1 != \"NA\" else \"NA\" for q3, q1 in zip(q3_value_list, q1_value_list)]\n",
        "        return range_value\n",
        "\n",
        "    range_value = find_range(min_value, max_value)\n",
        "    iqr_value = iqr(q1_value, q3_value)\n",
        "\n",
        "    # organise everything inside a dataframe.\n",
        "    df_dict = {}\n",
        "    df_dict[\"min\"] = min_value\n",
        "    df_dict[\"max\"] = max_value\n",
        "    df_dict[\"range(max-min)\"] = range_value\n",
        "    #df_dict[\"mode\"] = mode_value\n",
        "    df_dict[\"mean/average\"] = mean_value\n",
        "    df_dict[\"standard deviation\"] = std_value\n",
        "    df_dict[\"Q1\"] = q1_value\n",
        "    df_dict[\"meadian/Q2\"] = q2_meadian_value\n",
        "    df_dict[\"Q3\"] = q3_value\n",
        "    df_dict[\"Inter quantile range\"] = iqr_value\n",
        "    df_dict[\"kurtosis\"] = kurtosis_value\n",
        "    df_dict[\"Skewness\"] = skewness_value\n",
        "\n",
        "    df = pd.DataFrame(df_dict, columns = df_dict.keys(), index = column)\n",
        "\n",
        "    # highlight the data based on its skewness.\n",
        "    def highlight_skewness(sample):\n",
        "        # make a style as the sample shape and property.\n",
        "        style = sample.copy()\n",
        "        # make other cell_value as empty style , because i am focusing in coloring skewness column only.\n",
        "        style[:] = ''\n",
        "        # set the colors for skewness cells.\n",
        "        highly_skewed = 'background-color: red;'\n",
        "        moderatly_skewed = 'background-color: blue;'\n",
        "        perfect_normal_destribution = 'background-color: green;'\n",
        "\n",
        "        # color the cells\n",
        "        if sample[-1] > 1 or sample[-1] < -1:\n",
        "            style[-1] = highly_skewed\n",
        "        elif (sample[-1] > 0.5 or sample[-1]<=1) or (sample[-1] > -0.5 or sample[-1]<=-1):\n",
        "            style[-1] = moderatly_skewed\n",
        "        elif sample[-1] == 0:\n",
        "            style[:] = perfect_normal_destribution\n",
        "        else:\n",
        "            style[:] = ''\n",
        "        return style\n",
        "\n",
        "    df = df.style.apply(highlight_skewness, axis = 1)\n",
        "    display(df)\n",
        "\n",
        "\n",
        "\n",
        "numerical_data_analysis(data)"
      ],
      "metadata": {
        "id": "q9YhW8YZHqZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Feature Distribution of both train and test data"
      ],
      "metadata": {
        "id": "GQjwUtJsMn5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ncols = 5\n",
        "nrows = int(len(cont_features) / ncols + (len(FEATURES) % ncols > 0))-1\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(18, 10), facecolor='#EAEAF2')\n",
        "\n",
        "for r in range(nrows):\n",
        "    for c in range(ncols):\n",
        "        col = cont_features[r*ncols+c]\n",
        "        sns.histplot(x=train[col], ax=axes[r, c], color='#58D68D', label='Train data' , fill =True , kde = True)\n",
        "        sns.histplot(x=test[col], ax=axes[r, c], color='#DE3163', label='Test data', fill =True, kde = True)\n",
        "        axes[r,c].legend()\n",
        "        axes[r, c].set_ylabel('')\n",
        "        axes[r, c].set_xlabel(col, fontsize=8)\n",
        "        axes[r, c].tick_params(labelsize=5, width=0.5)\n",
        "        axes[r, c].xaxis.offsetText.set_fontsize(4)\n",
        "        axes[r, c].yaxis.offsetText.set_fontsize(4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uvIU9UcVMtFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Features train and test distribution"
      ],
      "metadata": {
        "id": "aBc3DHVqMzko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if len(cat_features) == 0 :\n",
        "    print(\"No Categorical features\")\n",
        "else:\n",
        "    ncols = 3\n",
        "    nrows = 1\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5))\n",
        "    for r in range(nrows):\n",
        "        for c in range(ncols):\n",
        "            col = cat_features[c]\n",
        "            sns.countplot(train[col],ax = axes[c] ,palette = \"viridis\", label='Train data')\n",
        "            sns.countplot(test[col],ax = axes[c] ,palette = \"magma\", label='Test data')\n",
        "            axes[c].legend()\n",
        "            axes[c].set_ylabel('')\n",
        "            axes[c].set_xlabel(col, fontsize=20)\n",
        "            axes[c].tick_params(labelsize=10, width=0.5)\n",
        "            axes[c].xaxis.offsetText.set_fontsize(4)\n",
        "            axes[c].yaxis.offsetText.set_fontsize(4)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kI6u76P3M66v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outlier Detection + discrete target"
      ],
      "metadata": {
        "id": "v8Nl9H2CJtDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cotinuous_column_list = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n",
        "                        'liveness', 'loudness', 'speechiness', 'tempo', 'audio_valence',\n",
        "                        ]\n",
        "fig, ax = plt.subplots(3,3, figsize=(15,20))\n",
        "row, col = 3, 3\n",
        "col_count = 0\n",
        "for r in range(row):\n",
        "    for c in range(col):\n",
        "        sns.boxplot(data=data, x=\"target\", y=cotinuous_column_list[col_count], ax=ax[r,c], palette=\"Set2\")\n",
        "        \n",
        "        col_count += 1"
      ],
      "metadata": {
        "id": "leaUl6JnJvmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding the number of outliers using different methods\n",
        "* for normal distributions use the Z score\n",
        "* for non-normal distributuions use the IQR"
      ],
      "metadata": {
        "id": "e6RUYZllKFoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find how much outliers it has using the IQR methods(if data skewed) and z-score method(if data is normally distributed).\n",
        "def find_outlier_z_score_method(data, new_feature=False, col_name=None):\n",
        "    \"\"\" Find the outliers in the given dataset. \n",
        "    :param data: dataset has number of features to find the outliers.\n",
        "    :param new_feature: If True create a new feature in the dataFrame to indicate this sample has any outlier feature else do nothing.\n",
        "\n",
        "    :return data with new feature and number of outlier in each features if new_feature is True, else only number of outlier in each features.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "    mean_each_features = df.mean(axis=0)\n",
        "    std_each_features = df.std(axis=0)\n",
        "    lower_limit_each_feature = mean_each_features - (3 * std_each_features)\n",
        "    upper_limit_each_feature = mean_each_features + (3 * std_each_features)\n",
        "\n",
        "    # find the data is a outlier value or not.\n",
        "    # print(lower_limit_each_feature, upper_limit_each_feature)\n",
        "    outlier_df = (df > upper_limit_each_feature) | (df < lower_limit_each_feature)\n",
        "    # find the number of outliers per feature.\n",
        "    number_of_outlier_each_feature = outlier_df.sum(axis= 0)\n",
        "    if df.ndim == 1:\n",
        "        # if the given data is features than handle diffrently. because it has no .values and .index function.\n",
        "        number_of_outlier_each_feature_df = pd.DataFrame({\"Feature\": [col_name if col_name else 'Given feature'], \"Number of outliers\":number_of_outlier_each_feature})\n",
        "    else:\n",
        "        number_of_outlier_each_feature_df = pd.DataFrame({\"Features\":number_of_outlier_each_feature.index, \"Number of outliers\": number_of_outlier_each_feature.values})\n",
        "\n",
        "    if new_feature:\n",
        "        # add the new feature indicating this row has outlier data to the data and return.\n",
        "        df[\"num_of_outliers\"] = outlier_df.sum(axis= 1)\n",
        "        return df, number_of_outlier_each_feature_df\n",
        "    return number_of_outlier_each_feature_df\n",
        "\n",
        "def find_outliers_iqr_method(data, new_feature=False, col_name=None):\n",
        "    \"\"\" Find the outliers in the given dataset. \n",
        "    :param data: dataset has number of features to find the outliers.\n",
        "    :param new_feature: If True create a new feature in the dataFrame to indicate this sample has any outlier feature else do nothing.\n",
        "\n",
        "    :return data with new feature and number of outlier in each features if new_feature is True, else only number of outlier in each features.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "    q1_each_features = df.quantile(0.25,)\n",
        "    q3_each_features = df.quantile(0.75,)\n",
        "    iqr_each_feature = q3_each_features - q1_each_features\n",
        "    lower_limit_each_feature = q1_each_features - (iqr_each_feature * 1.5)\n",
        "    upper_limit_each_feature = q3_each_features + (iqr_each_feature * 1.5)\n",
        "\n",
        "    # find the data is a outlier value or not.\n",
        "    # print(lower_limit_each_feature, upper_limit_each_feature)\n",
        "    outlier_df = (df > upper_limit_each_feature) | (df < lower_limit_each_feature)\n",
        "\n",
        "    # find the number of outliers per feature.\n",
        "    number_of_outlier_each_feature = outlier_df.sum(axis= 0)\n",
        "    if df.ndim == 1:\n",
        "        # if the given data is features than handle diffrently. because it has no .values and .index function.\n",
        "        number_of_outlier_each_feature_df = pd.DataFrame({\"Feature\": [col_name if col_name else 'Given feature'], \"Number of outliers\":number_of_outlier_each_feature})\n",
        "    else:\n",
        "        number_of_outlier_each_feature_df = pd.DataFrame({\"Features\":number_of_outlier_each_feature.index, \"Number of outliers\": number_of_outlier_each_feature.values})\n",
        "    if new_feature:\n",
        "        # add the new feature indicating this row has outlier data to the data and return.\n",
        "        df[\"num_of_outliers\"] = outlier_df.sum(axis= 1)\n",
        "        return df, number_of_outlier_each_feature_df\n",
        "    return number_of_outlier_each_feature_df\n",
        "    \n",
        "\n",
        "# find the number of outliers on each features.\n",
        "cotinuous_column_list = ['acousticness', 'danceability', 'energy',\n",
        "                        'liveness', 'loudness', 'speechiness', 'tempo', 'audio_valence',\n",
        "                        ] # we are not using the 'song_duration_ms' because it follows a normal distribution so need to use z-score method.\n",
        "outliers = find_outliers_iqr_method(data[cotinuous_column_list])\n",
        "display(outliers)\n",
        "find_outlier_z_score_method(data[\"song_duration_ms\"], col_name=\"song_duration_ms\") # col name is must if you giving a single feature for test\n",
        "\n"
      ],
      "metadata": {
        "id": "P_2Vkv-VKJ3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Data Distribution (univariate)"
      ],
      "metadata": {
        "id": "tOMCFBTBKulz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descrete_column_list = [ 'key', 'audio_mode','time_signature']\n",
        "fig, ax = plt.subplots(3,1, figsize = (8,14))\n",
        "row = 3\n",
        "col_count = 0\n",
        "for r in range(row):\n",
        "    sns.countplot(data=data, x=descrete_column_list[col_count], ax=ax[r])\n",
        "    col_count +=1"
      ],
      "metadata": {
        "id": "71GBHwvRK0nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Analysis"
      ],
      "metadata": {
        "id": "THPmI4seLA4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# can be done by heatmap.\n",
        "fig = plt.figure(figsize = (18,12))\n",
        "sns.heatmap(data=data.corr(), annot=True, vmin=0, vmax=1,)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "D2Rv7di2LDxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}