{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnzp0iN1QIH/krjC/MxGdK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koleshjr/ALL_MY_TEMPLATES/blob/main/NLP_Pipeline_MultiClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP MULTICLASSIFICATION PIPELINE - ITERATIVE EXPERIMENTS"
      ],
      "metadata": {
        "id": "8NaAK0LMc5RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n",
        "!pip install transformers\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "Das3J74feFuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "eEJehUjCc95u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "path = Path('/content/drive/MyDrive/Swahili_nlp')"
      ],
      "metadata": {
        "id": "TWkRT4kRdktb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.disable(logging.WARNING)"
      ],
      "metadata": {
        "id": "oHNceRfjdls5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df = pd.read_csv(path/'train.csv')\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "73zHSTIZdsih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = pd.read_csv(path/'test.csv')\n",
        "len(eval_df)"
      ],
      "metadata": {
        "id": "p4L4CjJ4dvjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df.head()"
      ],
      "metadata": {
        "id": "olwJ7vxPdy4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.value_counts()\n"
      ],
      "metadata": {
        "id": "OKwSGQ7nd1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "1T9pK_mdd7eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import warnings,transformers,logging,torch\n",
        "from transformers import TrainingArguments,Trainer\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer"
      ],
      "metadata": {
        "id": "-V_Chosod85g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict"
      ],
      "metadata": {
        "id": "62b1A2STeBcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nm = 'microsoft/deberta-v3-small'"
      ],
      "metadata": {
        "id": "qDufVOUUeRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert pandas dataframes to hugging face dataframe"
      ],
      "metadata": {
        "id": "UaaNtdBxeam0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_pandas(df).rename_column('score', 'label')\n",
        "eval_ds = Dataset.from_pandas(eval_df)"
      ],
      "metadata": {
        "id": "qGuf6IVceZlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing function"
      ],
      "metadata": {
        "id": "EfgTvo-1eghn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tok_func(x): return tokz(x[\"inputs\"])"
      ],
      "metadata": {
        "id": "MsoTEIUQeh3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_func(ds[0]"
      ],
      "metadata": {
        "id": "wlb9JUEeeluO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize and remove the columns we no longer need \n",
        "inps = \"anchor\",\"target\",\"context\"\n",
        "tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))"
      ],
      "metadata": {
        "id": "t0OSa6XseriX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at the first item"
      ],
      "metadata": {
        "id": "sNdL-CsAfF1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ds[0]"
      ],
      "metadata": {
        "id": "zuYaveezfIUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a validation set"
      ],
      "metadata": {
        "id": "Ucnw6n6-fLEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_idxs, val_idxs = train_test_split(df, test_size=0.2, stratify=df['target_column'])\n",
        "train_idxs = list(train_idxs)\n",
        "val_idxs = list(val_idxs)\n",
        "len(val_idxs),len(train_idxs)"
      ],
      "metadata": {
        "id": "VkW3ZOSFfNBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the mean of both traain and val is from the same distribution"
      ],
      "metadata": {
        "id": "5Cv063LggsTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[trn_idxs].score.mean(),df.iloc[val_idxs].score.mean()"
      ],
      "metadata": {
        "id": "Fr_DbYbrgycR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Hugging Face Datasets\n",
        "dds = DatasetDict({\"train\":tok_ds.select(trn_idxs),\n",
        "             \"test\": tok_ds.select(val_idxs)})\n",
        "\n"
      ],
      "metadata": {
        "id": "BW1wzOZ3gSra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Model"
      ],
      "metadata": {
        "id": "0Q0vXWSuhLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_accuracy(eval_pred): return {'multiclass_accuracy': np.mean(eval_pred[0] == eval_pred[1])}\n",
        "\n",
        "def multiclass_f1(eval_pred): \n",
        "    true_positives = np.sum((eval_pred[0] == eval_pred[1]) & (eval_pred[0] == 1))\n",
        "    false_positives = np.sum((eval_pred[0] != eval_pred[1]) & (eval_pred[1] == 1))\n",
        "    false_negatives = np.sum((eval_pred[0] != eval_pred[1]) & (eval_pred[0] == 1))\n",
        "    precision = true_positives/(true_positives + false_positives)\n",
        "    recall = true_positives/(true_positives + false_negatives)\n",
        "    return {'multi_class_f1_score': 2*((precision*recall)/(precision + recall)"
      ],
      "metadata": {
        "id": "W6FUXS6ahMsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model params"
      ],
      "metadata": {
        "id": "ALctm58lhxy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr,bs = 8e-5,128\n",
        "wd,epochs = 0.01,4"
      ],
      "metadata": {
        "id": "V8yWjAVKhyqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers uses the TrainingArguments class to set up arguments. We'll use a cosine scheduler with warmup, since at fast.ai we've found that's pretty reliable. We'll use fp16 since it's much faster on modern GPUs, and saves some memory. We evaluate using double-sized batches, since no gradients are stored so we can do twice as many rows at a time."
      ],
      "metadata": {
        "id": "Kg11v_l_h84F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainer(dds):\n",
        "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
        "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
        "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                   tokenizer=tokz, compute_metrics=[multiclass_accuracy, multiclass_f1])"
      ],
      "metadata": {
        "id": "y4qcp3Nmh7Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
        "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "    num_train_epochs=epochs, weight_decay=wd, report_to='none')"
      ],
      "metadata": {
        "id": "d-be8wweiGNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
        "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "               tokenizer=tokz, compute_metrics=corr)"
      ],
      "metadata": {
        "id": "HCrmN5c9iRJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train();"
      ],
      "metadata": {
        "id": "PpPHyVg3iYIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving the Model"
      ],
      "metadata": {
        "id": "DXkkZw8Sicbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to start iterating to improve this. To do that, we need to know whether the model gives stable results. I tried training it 3 times from scratch, and got a range of outcomes from 0.808-0.810. This is stable enough to make a start - if we're not finding improvements that are visible within this range, then they're not very significant! Later on, if and when we feel confident that we've got the basics right, we can use cross validation and more epochs of training."
      ],
      "metadata": {
        "id": "ox4FrrWlicoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geet dataset function"
      ],
      "metadata": {
        "id": "m7llUMIoizw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dds(df):\n",
        "    ds = Dataset.from_pandas(df).rename_column('score', 'label')\n",
        "    tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('inputs','id','section'))\n",
        "    return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})"
      ],
      "metadata": {
        "id": "CTUH_JV1ioUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer Function"
      ],
      "metadata": {
        "id": "tTlb0rUWi2WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(): return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=5)\n",
        "\n",
        "def get_trainer(dds, model=None):\n",
        "    if model is None: model = get_model()\n",
        "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
        "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
        "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                   tokenizer=tokz, compute_metrics=[multiclass_accuracy, multiclass_f1])"
      ],
      "metadata": {
        "id": "KQQ4eJUJi4K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering + Preprocessing if necessary"
      ],
      "metadata": {
        "id": "W3QoX-dAi8Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sep = \" [s] \"\n",
        "df['inputs'] = df.context + sep + df.anchor + sep + df.target\n",
        "dds = get_dds(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "oJM85EmjjB0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_trainer(dds).train()"
      ],
      "metadata": {
        "id": "klOddXbBjFwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['inputs'] = df.inputs.str.lower()\n",
        "dds = get_dds(df)\n",
        "get_trainer(dds).train()"
      ],
      "metadata": {
        "id": "2VPJxhw9jJsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Validation"
      ],
      "metadata": {
        "id": "o40QdvtPjN9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_folds = 4\n"
      ],
      "metadata": {
        "id": "240bvlJSjPd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "cv = StratifiedGroupKFold(n_splits=n_folds)"
      ],
      "metadata": {
        "id": "j-jRuLT7jUyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1, random_state=42)\n",
        "scores = (df.score*100).astype(int)\n",
        "folds = list(cv.split(idxs, scores, df.anchor))\n",
        "folds"
      ],
      "metadata": {
        "id": "NxuDkSTLjZlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Function to create Folds"
      ],
      "metadata": {
        "id": "SyY3p9nxjfoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fold(folds, fold_num):\n",
        "    trn,val = folds[fold_num]\n",
        "    return DatasetDict({\"train\":tok_ds.select(trn), \"test\": tok_ds.select(val)})"
      ],
      "metadata": {
        "id": "gFQxCyFgje3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dds = get_fold(folds, 0)\n",
        "dds"
      ],
      "metadata": {
        "id": "dA0i1Y9YjlvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We can now pass this into get_trainer as we did before. If we have, say, 4 folds, then doing that for each fold will give us 4 models, and 4 sets of predictions and metrics. You could ensemble the 4 models to get a stronger model, and can also average the 4 metrics to get a more accurate assessment of your model. Here's how to get the final epoch metrics from a trainer:\n"
      ],
      "metadata": {
        "id": "fHb5nEmBjurV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [o['eval_pearson'] for o in trainer.state.log_history if 'eval_pearson' in o]\n",
        "metrics[-1]"
      ],
      "metadata": {
        "id": "zY8Yd96Fjrcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "5vAXgmJAj2XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
        "preds\n"
      ],
      "metadata": {
        "id": "e7GWBHyIj3ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.clip(preds, 0, 1)"
      ],
      "metadata": {
        "id": "7xLfOfC9kXU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "778lk82rkai3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "submission = datasets.Dataset.from_dict({\n",
        "    'id': eval_ds['id'],\n",
        "    'score': preds\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "Hxk2E0rLkfAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameter Tuning"
      ],
      "metadata": {
        "id": "KFGpikoa62Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the search space\n",
        "import optuna \n",
        "\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
        "    bs = trial.suggest_int('bs', 32, 512)\n",
        "    epochs = trial.suggest_int('epochs', 10, 100)\n",
        "    wd = trial.suggest_loguniform('wd', 1e-10, 1e-2)\n",
        "    \n",
        "    #Define the model\n",
        "    model = get_model()\n",
        "    #Define the training arguments\n",
        "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
        "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
        "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
        "    trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
        "                   tokenizer=tokz, compute_metrics=[multiclass_accuracy, multiclass_f1])\n",
        "    #Train the model\n",
        "    trainer.train()\n",
        "    #Evaluate the model\n",
        "    score = trainer.evaluate()\n",
        "    return score\n",
        "\n",
        "#Run the optimization loop\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=20)\n",
        "#Print the best parameters\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "u09euQdL8C6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}